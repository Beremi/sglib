\documentclass{article}
\usepackage{amsmath,amssymb,epsfig,bbm}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\tmmathbf}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\title{Representation of Random Field for Stochastic Galerkin
Methods}\author{Elmar Zander}\maketitle

\begin{figure}[h]
  \epsfig{file=oberseminar-1.eps}
  \caption{}
\end{figure}

{\newpage}

\section{Deterministic PDE}

\[ \mathcal{L} u = f \]
Example: Groundwater flow (stationary)
\[ - \nabla \cdot \left( \kappa (x) \nabla u (x)) = f (x) \right. \]
Galerkin approach: $u^h = \sum_i u_i \varphi_i (x)$ with $\varphi_i \in V^h
\subseteq H^1_E$ require
\[ \left( \mathcal{L} u^h, v^h) = (f, v^h) \tmop{for} \tmop{each} v^h \in V^h
   \right. \]
Result: linear system
\[ K \tmmathbf{u} = \tmmathbf{f} \]
with $K_{\tmop{ij}} = (\kappa (x) \nabla \varphi_i (x), \nabla \varphi_j (x))$
and $\tmmathbf{u} = \left( u_1, \ldots \right)$ and $\tmmathbf{f} = (f_1,
\ldots)$ with $f_i = \left( f (x), \varphi_i (x) \right)$







{\newpage}

\section{Stochastic PDE}

\[ - \nabla \cdot \left( \kappa (x, \omega) \nabla u (x, \omega)) = f (x,
   \omega) \right. \]
with $\omega \in \Omega$ and $\kappa (x, \omega)$ and $f (x, \omega)$ random
fields ($\kappa, f : \mathcal{D} \times \Omega \rightarrow \mathbbm{R})$.

\begin{figure}[h]
  \epsfig{file=oberseminar-1.eps}
  \caption{Left $\kappa (\cdot, \omega_0)$, right pdf of $\kappa (x_0,
  \cdot)$}
\end{figure}

{\newpage}

\section{Stochastic Galerkin approach }

$u^h = \sum_{i, \alpha} u_{i, \alpha} \varphi_i (x) \Psi_{\alpha} (\omega)$
with $\varphi_i \in V^h \subseteq H^1_E$ and $\Psi_{\alpha} \in \mathcal{S}
\subseteq L_2 (\Omega, \mathcal{F}, P)$ require
\[ \mathbbm{E} \left[ \left( \mathcal{L} u^h, v^h) \Psi (\cdot) \right] =
   \mathbbm{E} \left[ (f, v^h) \Psi (\cdot) \right] \tmop{for} \tmop{each} v^h
   \in V^h \tmop{and} \Psi \in \mathcal{S} \right. \]
Result: linear system
\[ K \tmmathbf{u} = \tmmathbf{f} \]
which really means
\[ K_{\tmop{ij} \alpha \beta} u_{j \beta} = f_{i \alpha} \]
with $\left. K_{\tmop{ij} \alpha \beta} = \mathbbm{E} [(\kappa (x, \omega)
\nabla \varphi_i (x), \nabla \varphi_j (x)) \Psi_{\alpha} (\omega)
\Psi_{\beta} (\omega) \right]$ and $\left. f_{i \alpha} = \mathbbm{E} [ \left(
f (x, \omega), \varphi_j (x) \right) \Psi_{\alpha} (\omega) \right]$

{\newpage}

\section{Representation of parameter fields}

What is $\kappa (x, \omega)$ (or $f (x, \omega)$) and how do we represent it?
\[  \]
Series expansion
\[ f (x, \omega) = \sum_i v^f_i (x) \xi^f_i (\omega) \]
then expand the stochastic variable
\[ \xi^f_i (\omega) = \sum_{\alpha} \xi_{i, \alpha} \Psi_{\alpha} (\theta_1
   (\omega), \theta_2 (\omega), \ldots) \]
where $\theta_i$ are some iid random variables



Only information available: marginal densities $\mathbbm{P} [f (x, \cdot)
\leqslant y]$ and covariances $\tmop{cov}_f (x_1, x_2) = \mathbbm{E} [(f (x_1,
\cdot) - \bar{f} (x_1)) (f (x_2, \cdot) - \bar{f} (x_2))]$. Not sufficient for
general random field, but for Gaussian random field



Assumption $f (x, \omega)$ is a translated Gaussian field, i.e.
\[ f (x, \omega) = \Phi (x, \gamma (x, \omega)) \]


{\newpage}

\section{The polynomial chaos expansion}

Random variable $X (\omega) \in L_2 (\Omega, \Sigma, P)$, represent by
subspace of Gaussian variables ($\rightarrow$Gaussian Hilbert space) $\Theta
\subseteq L_2 (\Omega, \Sigma, P)$, $\gamma_1, \gamma_2, \ldots \in \Theta$
and independent i.e. $\tmmathbf{\alpha} = (\alpha_1, \alpha_2, \ldots)$
multiindex, then
\[ X (\omega) = \sum_{\alpha} x_{\alpha} H_{\alpha} (\gamma_1, \gamma_2,
   \ldots) \]
with
\[ x_{\alpha} = \mathbbm{E} [X H_{\alpha}] / \mathbbm{E} [H_{\alpha}^2] \]
especially $X (\omega) \in L_2 ( \mathbbm{R}, \mathcal{B}, \Gamma)$ then
\[ X (\omega) = \sum_i x_i H_i (\gamma) \]
where $H_i$ are the univariate Hermite polynomials, $H_{\alpha} (\gamma_1,
\gamma_2, \ldots) = H_{\alpha_1} (\gamma_1) H_{\alpha_2} (\gamma_2) \cdots$



Suppose $X \sim \tmop{LogN} (2, 3)$ then $X (\omega) = \exp (2 + 0.5 \gamma
(\omega))$
\[ x_j = \frac{1}{j!} \int_{\mathbbm{R}} \exp (2 + 0.5 s) H_j (s) \frac{e^{-
   s^2 / 2}}{\sqrt{2 \pi}}  d s \]
\begin{verbatim}
X={@lognorm_stdnor, {2, 0.5} }; 
x_i=pce_expand_1d( X, 4 );
hermite_val( x_i, randn(20000,1) );
\end{verbatim}
\begin{figure}[h]
  \epsfig{file=oberseminar-3.eps}
  \caption{Approximation of transformation function and of pdf (sampled)}
\end{figure}

{\newpage}

\begin{figure}[h]
  \epsfig{file=oberseminar-4.eps}
  \caption{Approximation of beta distribution, p=0,1,2,3}
\end{figure}

{\newpage}

\begin{figure}[h]
  \epsfig{file=oberseminar-5.eps}
  \caption{Approximation of uniform distribution, p=1,3,5,7}
\end{figure}

{\newpage}

\begin{figure}[h]
  \epsfig{file=oberseminar-6.eps}
  \caption{Approximation of a bimodal distribution, p=2,4,8,11}
\end{figure}

{\newpage}

\section{Karhunen-Lo\`eve expansion (KL)}

Given covariance $\tmop{cov}_f (x_1, x_2)$ define
\[ (T \varphi) (x') = \int_{\mathcal{D}} \tmop{cov}_f (x, x') \varphi (x) d x'
\]
Solve eigenproblem
\[ T v^f_i = \lambda_i v^f_i \]
Representation for $f$
\[ f (x, \omega) = \bar{f} (x) + \sum_i \sqrt{\lambda_i} v_i^f (x) \xi_i^f
   (\omega) \]
Optimal in some least squares sense,

\begin{figure}[h]
  \epsfig{file=presentation_oberseminar_07.eps}
  \caption{Gaussian, exponential and spherical covariance}
\end{figure}

{\newpage}


\begin{verbatim}
cov_u=@(x1,x2)(gaussian_covariance(x1,x2,[0.7 0.3])); 
C_u=covariance_matrix( x, cov_u );
v_u=kl_expand( C_u, [], 4, 'correct_var', true );
plot_field( els, pos, v_u(1,:) )
\end{verbatim}
\begin{figure}[h]
  \epsfig{file=presentation_oberseminar_08.eps}
  \caption{KL-eigenmodes of anisotropic Gaussian covariance}
\end{figure}

{\newpage}

\section{Putting it together}

Lokal PC expansion in terms of the base field
\[ f (x, \omega) = \sum_{i = 0}^{\infty} f_i (x) H_i (\gamma (x, \omega))
   \label{eq:saka-expand-uU} \]
\[ \gamma (x, \omega) = \sum_{i = 1}^{\infty} \sqrt{\lambda_i} v^{\gamma}_i
   (x) \theta_i \]
Global PC expansion in terms of the independent Gaussians
\[ f (x, \omega) = \sum_{\alpha \in \mathcal{J}} f_{\alpha} (x) H_{\alpha} (
   \tmmathbf{\theta}) \label{eq:saka-expand-uu} \]
``Sakamoto's'' formula
\[ H_i ( \tmmathbf{a}^T \tmmathbf{\xi}) = \sum_{\alpha, | \alpha \vDash i}
   \frac{i!}{\alpha !} a^{\alpha} H_{\alpha} ( \tmmathbf{\xi}) \hspace{1em}
   (\tmop{if} \| \tmmathbf{a} \|= 1) \]
leads to
\[ f_{\alpha} (x) = \frac{| \alpha |!}{\alpha !} f_{| \alpha |}
   \prod^{\tmop{len} (\alpha)}_{j = 1^{}} \left[ \sqrt{\lambda_j} v^{\gamma}_j
   (x) \right]^{\alpha_j} \]
{\pagebreak}
\begin{verbatim}
f_i=pce_expand_1d(h,p);
C_f=covariance_matrix( x, cov_f );
C_gam=transform_covariance_pce( C_f, f_i, transform_options );
v_gam=kl_expand( C_gam, M, m_gam, options );
[f_alpha,I_f]=pce_transform_multi( v_gam, f_i );

f_ex=kl_pce_field_realization( pos, mu_f, v_f, f_i_alpha, I_f );
plot_field( els, pos, f_ex );
\end{verbatim}
\begin{figure}[h]
  \epsfig{file=presentation_oberseminar_09.eps}
  \caption{}
\end{figure}

\\
\section{Analysis of random field}
\begin{verbatim}
[u_alpha, I_u]=expand_field_pce_sg( h, cov_u, cov_gam, x, M, p, m ); C_u2=pce_covariance( u_alpha, I_u );
\end{verbatim}
\begin{figure}[h]
  \epsfig{file=presentation_oberseminar_10.eps}
  \caption{Sample realizations of random field with beta(4,2) distribution and
  Gaussian covariance}
\end{figure}

{\newpage}

\begin{figure}[h]
  \epsfig{file=presentation_oberseminar_11.eps}
  \caption{Comparison of original covariance and PCE covariance}
\end{figure}

\begin{figure}[h]
  \epsfig{file=presentation_oberseminar_12.eps}
  \caption{Comparison of specified marginal density and PCE marginal density}
\end{figure}



{\newpage}

\section{Not covered \& further work}



Sparse representation by KL on target field (already implemented)



Construction of stochastic operators (implemented but not fully tested)



Solvers for tensor product representations (partially implemented)



Representation of bimodal distributions and non-translational fields.



Speed, speed, speed!





\end{document}
